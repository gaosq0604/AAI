{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10回講義 演習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題1. Recurrent Neural Network (RNN) Encoder-Decoderモデルで英日翻訳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. データセットの読み込みと単語・品詞のID化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. データセットについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.enとtrain.jaの中身は次のようになっています.\n",
    "\n",
    "train.enの中身 (英語の文)\n",
    "```\n",
    "i can 't tell who will arrive first .\n",
    "many animals have been destroyed by men .\n",
    "i 'm in the tennis club .\n",
    "︙\n",
    "```\n",
    "\n",
    "train.jaの中身(日本語の文, 対訳)\n",
    "```\n",
    "誰 が 一番 に 着 く か 私 に は 分か り ま せ ん 。\n",
    "多く の 動物 が 人間 に よ っ て 滅ぼ さ れ た 。\n",
    "私 は テニス 部員 で す 。\n",
    "︙\n",
    "```\n",
    "(データセットにはTanaka Corpus ( http://www.edrdg.org/wiki/index.php/Tanaka_Corpus )の一部を抽出した \n",
    "small_parallel_enja: 50k En/Ja Parallel Corpus for Testing SMT Methods ( https://github.com/odashi/small_parallel_enja ) を使っています.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. 単語・品詞のID化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語のままだと扱いづらいので, それぞれの単語をIDに置き換えます. 以下のコードでは, まず`build_vocab`で単語->idの辞書(`w2i`)を作り, それを元に`encode`で各単語をid化しています."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(file_path):\n",
    "    vocab = set()\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        words = line.strip().split()\n",
    "        vocab.update(words)\n",
    "\n",
    "    w2i = {w: np.int32(i+2) for i, w in enumerate(vocab)}\n",
    "    w2i['<s>'], w2i['</s>'] = np.int32(0), np.int32(1) # 文の先頭・終端記号\n",
    "\n",
    "    return w2i\n",
    "\n",
    "def encode(sentence, w2i):\n",
    "    encoded_sentence = []\n",
    "    for w in sentence:\n",
    "        encoded_sentence.append(w2i[w])\n",
    "    return encoded_sentence\n",
    "\n",
    "def load_data(file_path, vocab=None, w2i=None):\n",
    "    if vocab is None and w2i is None:\n",
    "        w2i = build_vocab(file_path)\n",
    "    \n",
    "    data = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        s = line.strip().split()\n",
    "        s = ['<s>'] + s + ['</s>']\n",
    "        enc = encode(s, w2i)\n",
    "        data.append(enc)\n",
    "    i2w = {i: w for w, i in w2i.items()}\n",
    "    return data, w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 英語->日本語\n",
    "train_X, e_w2i, e_i2w = load_data('train.en')\n",
    "train_y, j_w2i, j_i2w = load_data('train.ja')\n",
    "\n",
    "train_X, _, train_y, _ = train_test_split(train_X, train_y, test_size=0.5, random_state=42) # 演習用に縮小\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.02, random_state=42)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.02, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 各層クラスの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のクラスの中では, 系列中の全てのステップに対してまとめて処理をおこなう`f_prop`関数の他に, 1つのステップに対してのみ処理をおこなう`f_prop_test`関数を実装しています. 理由は後述します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. 単語のEmbedding層\n",
    "\n",
    "$m$ : emb_dim\n",
    "\n",
    "$n$ : vocab_size\n",
    "\n",
    "実際にEmbedding層の処理を担うembedding_lookupでは, 入力をone_hotベクトルに変換し, Embedding層の行列に掛け, 対応する列ベクトルを選択します.\n",
    "![embedding](embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, vocab_size, emb_dim, scale=0.08):\n",
    "        self.V = tf.Variable(rng.randn(vocab_size, emb_dim).astype('float32') * scale, name='V')\n",
    "\n",
    "    def f_prop(self, x):\n",
    "        return tf.nn.embedding_lookup(self.V, x)\n",
    "    \n",
    "    def f_prop_test(self, x_t):\n",
    "        return tf.nn.embedding_lookup(self.V, x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装する式は次のようになります. ($\\odot$は要素ごとの積)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 入力ゲート: $\\hspace{20mm}i_t = \\sigma \\left( W_{xi} x_t + W_{hi} h_{t-1} + b_i \\right)$\n",
    "- 忘却ゲート: $\\hspace{20mm}f_t = \\sigma \\left( W_{xf} x_t + W_{hf} h_{t-1} + b_f \\right)$  \n",
    "- 出力ゲート: $\\hspace{20mm}o_t = \\sigma \\left( W_{xo} x_t + W_{ho} h_{t-1} + b_o \\right)$  \n",
    "- セル:　　　 $\\hspace{20mm}c_t = f_t \\odot c_{t-1} + i_t \\odot \\tanh \\left( W_{xc} x_t + W_{hc} h_{t-1} + b_c \\right)$  \n",
    "- 隠れ層: 　　$\\hspace{20mm}h_t = o_t \\odot \\tanh \\left( c_t \\right)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単純なRNNでは各ステップの関数の戻り値は隠れ層のみ ($h_t$) でしたが, LSTMでは隠れ層とセル状態の2つ ($h_t, c_t$) となるので注意してください. またマスクに関しても両方に適用する必要があります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, in_dim, hid_dim, m, h_0=None, c_0=None):\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        # input gate\n",
    "        self.W_xi = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xi')\n",
    "        self.W_hi = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_hi')\n",
    "        self.b_i  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_i')\n",
    "        \n",
    "        # forget gate\n",
    "        self.W_xf = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xf')\n",
    "        self.W_hf = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xf')\n",
    "        self.b_f  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_f')\n",
    "\n",
    "        # output gate\n",
    "        self.W_xo = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xo')\n",
    "        self.W_ho = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_ho')\n",
    "        self.b_o  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_o')\n",
    "\n",
    "        # cell state\n",
    "        self.W_xc = tf.Variable(tf.random_uniform([in_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_xc')\n",
    "        self.W_hc = tf.Variable(tf.random_uniform([hid_dim, hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='W_hc')\n",
    "        self.b_c  = tf.Variable(tf.random_uniform([hid_dim], minval=-0.08, maxval=0.08, dtype=tf.float32), name='b_c')\n",
    "\n",
    "        # initial state\n",
    "        self.h_0 = h_0\n",
    "        self.c_0 = c_0\n",
    "\n",
    "        # mask\n",
    "        self.m = m\n",
    "\n",
    "    def f_prop(self, x):\n",
    "        def fn(tm1, x_and_m):\n",
    "            h_tm1 = tm1[0]\n",
    "            c_tm1 = tm1[1]\n",
    "            x_t = x_and_m[0]\n",
    "            m_t = x_and_m[1]\n",
    "            # input gate\n",
    "            i_t = tf.nn.relu(tf.matmul(x_t, self.W_xi) + tf.matmul(h_tm1, self.W_hi) + self.b_i)\n",
    "\n",
    "            # forget gate\n",
    "            f_t = tf.nn.relu(tf.matmul(x_t, self.W_xf) + tf.matmul(h_tm1, self.W_hf) + self.b_f)\n",
    "\n",
    "            # output gate\n",
    "            o_t = tf.nn.relu(tf.matmul(x_t, self.W_xo) + tf.matmul(h_tm1, self.W_ho) + self.b_o)\n",
    "\n",
    "            # cell state\n",
    "            c_t = f_t * c_tm1 + i_t * tf.nn.tanh(tf.matmul(x_t, self.W_xc) + tf.matmul(h_tm1, self.W_hc) + self.b_c)\n",
    "            c_t = m_t[:, np.newaxis] * c_t + (1. - m_t[:, np.newaxis]) * c_tm1 # Mask\n",
    "\n",
    "            # hidden state\n",
    "            h_t = o_t * tf.nn.tanh(c_t)\n",
    "            h_t = m_t[:, np.newaxis] * h_t + (1. - m_t[:, np.newaxis]) * h_tm1 # Mask\n",
    "\n",
    "            return [h_t, c_t]\n",
    "\n",
    "        _x = tf.transpose(x, perm=[1, 0, 2])\n",
    "        _m = tf.transpose(self.m)\n",
    "\n",
    "        if self.h_0 == None:\n",
    "            self.h_0 = tf.matmul(x[:, 0, :], tf.zeros([self.in_dim, self.hid_dim]))\n",
    "        if self.c_0 == None:\n",
    "            self.c_0 = tf.matmul(x[:, 0, :], tf.zeros([self.in_dim, self.hid_dim]))\n",
    "\n",
    "        h, c = tf.scan(fn=fn, elems=[_x, _m], initializer=[self.h_0, self.c_0])\n",
    "        return tf.transpose(h, perm=[1, 0, 2]), tf.transpose(c, perm=[1, 0, 2])\n",
    "    \n",
    "    def f_prop_test(self, x_t):\n",
    "        # input gate\n",
    "        i_t = tf.nn.relu(tf.matmul(x_t, self.W_xi) + tf.matmul(self.h_0, self.W_hi) + self.b_i)\n",
    "\n",
    "        # forget gate\n",
    "        f_t = tf.nn.relu(tf.matmul(x_t, self.W_xf) + tf.matmul(self.h_0, self.W_hf) + self.b_f)\n",
    "\n",
    "        # output gate\n",
    "        o_t = tf.nn.relu(tf.matmul(x_t, self.W_xo) + tf.matmul(self.h_0, self.W_ho) + self.b_o)\n",
    "\n",
    "        # cell state\n",
    "        c_t = f_t * self.c_0 + i_t * tf.nn.tanh(tf.matmul(x_t, self.W_xc) + tf.matmul(self.h_0, self.W_hc) + self.b_c)\n",
    "\n",
    "        # hidden state\n",
    "        h_t = o_t * tf.nn.tanh(c_t)\n",
    "\n",
    "        return [h_t, c_t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. 全結合層"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f_prop`における入力は3階テンソルとなるので, `tf.einsum`を使用します.\n",
    "\n",
    "入力`x`と重み`W`のshapeは, それぞれ\n",
    "\n",
    "- `x`: (ミニバッチサイズ, i) x (系列長, j) x (入力次元数, k)\n",
    "- `W`: (入力次元数, k) x (出力次元数, l)\n",
    "\n",
    "で, 出力は\n",
    "\n",
    "- (ミニバッチサイズ, i) x (系列長, j) x (出力次元数, l)\n",
    "\n",
    "となるので, `einsum`の第一引数の表記は`'ijk,kl->ijl'`となります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, in_dim, out_dim, function=lambda x: x):\n",
    "        # Xavier\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                        low=-np.sqrt(6/(in_dim + out_dim)),\n",
    "                        high=np.sqrt(6/(in_dim + out_dim)),\n",
    "                        size=(in_dim, out_dim)\n",
    "                    ).astype('float32'), name='W')\n",
    "        self.b = tf.Variable(tf.zeros([out_dim], dtype=tf.float32), name='b')\n",
    "        self.function = function\n",
    "\n",
    "    def f_prop(self, x):\n",
    "        return self.function(tf.einsum('ijk,kl->ijl', x, self.W) + self.b)\n",
    "\n",
    "    def f_prop_test(self, x_t):\n",
    "        return self.function(tf.matmul(x_t, self.W) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 計算グラフ構築 & パラメータの更新設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下の図のモデルを実装します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![seq2seq](seq2seq.png)\n",
    "\n",
    "https://www.tensorflow.org/tutorials/seq2seq より引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ミニバッチサイズ, 系列長, 辞書のサイズをそれぞれ$N$, $T$, $K$とすると, 多クラス交差エントロピー誤差関数は次のようになります.\n",
    "\n",
    "$$\n",
    "    E({\\bf \\theta}) = -\\frac{1}{N}\\sum^N_{n=1}\\sum^T_{t=1}\\sum^K_{k=1} d^{(n)}_{t, k} \\log y^{(n)}_{t, k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder, Decoderともに短い系列に対してはpaddingをします. また, Encoderではマスクを行います.\n",
    "\n",
    "Decoderでpaddingした部分については, コストがゼロになるようにします. これは, 単語がある部分を1, paddingの部分を0とするバイナリのマスクをかけるか, paddingの部分の教師ラベルdの要素をすべてゼロになるようにします.\n",
    "\n",
    "`tf`においては, `tf.one_hot`でone_hot化するときに範囲外の値(-1など)を入力とすれば, その値に対するベクトルはすべてゼロとなります. 以下ではこちらの方法で実装しています."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e_vocab_size = len(e_w2i)\n",
    "j_vocab_size = len(j_w2i)\n",
    "emb_dim = 256\n",
    "hid_dim = 256\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, None], name='x')\n",
    "m = tf.cast(tf.not_equal(x, -1), tf.float32)\n",
    "d = tf.placeholder(tf.int32, [None, None], name='d')\n",
    "d_in = d[:, :-1]\n",
    "\n",
    "d_out = d[:, 1:]\n",
    "d_out_one_hot = tf.one_hot(d_out, depth=j_vocab_size, dtype=tf.float32)\n",
    "\n",
    "def f_props(layers, x):\n",
    "    for layer in layers:\n",
    "        x = layer.f_prop(x)\n",
    "    return x\n",
    "\n",
    "encoder = [\n",
    "    Embedding(e_vocab_size, emb_dim),\n",
    "    LSTM(emb_dim, hid_dim, m)\n",
    "]\n",
    "\n",
    "h_enc, c_enc = f_props(encoder, x)\n",
    "\n",
    "decoder_pre = [\n",
    "    Embedding(j_vocab_size, emb_dim),\n",
    "    LSTM(emb_dim, hid_dim, tf.ones_like(d_in, dtype='float32'), h_0=h_enc[:, -1, :], c_0=c_enc[:, -1, :]),\n",
    "]\n",
    "\n",
    "decoder_post = [\n",
    "    Dense(hid_dim, j_vocab_size, tf.nn.softmax)\n",
    "]\n",
    "\n",
    "h_dec, c_dec = f_props(decoder_pre, d_in)\n",
    "y = f_props(decoder_post, h_dec)\n",
    "\n",
    "cost = -tf.reduce_mean(tf.reduce_sum(d_out_one_hot * tf.log(tf.clip_by_value(y, 1e-10, 1.0)), axis=[1, 2]))\n",
    "\n",
    "train = tf.train.AdamOptimizer().minimize(cost)\n",
    "#train = tf.train.RMSPropOptimizer(1e-3).minimize(cost)\n",
    "#train = tf.train.RMSPropOptimizer(0.1).minimize(cost)\n",
    "#train = tf.train.MomentumOptimizer(1e-4,0.99, use_nesterov = True).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_lens = [len(com) for com in train_X]\n",
    "sorted_train_indexes = sorted(range(len(train_X_lens)), key=lambda x: -train_X_lens[x])\n",
    "\n",
    "train_X = [train_X[ind] for ind in sorted_train_indexes]\n",
    "train_y = [train_y[ind] for ind in sorted_train_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Training cost: 67.544, Validation cost: 44.162, Time: 31\n",
      "EPOCH: 2, Training cost: 38.479, Validation cost: 38.435, Time: 58\n",
      "EPOCH: 3, Training cost: 34.704, Validation cost: 36.597, Time: 86\n",
      "EPOCH: 4, Training cost: 32.158, Validation cost: 36.331, Time: 115\n",
      "EPOCH: 5, Training cost: 30.337, Validation cost: 36.147, Time: 143\n",
      "EPOCH: 6, Training cost: 28.875, Validation cost: 35.707, Time: 171\n",
      "EPOCH: 7, Training cost: 27.961, Validation cost: 36.590, Time: 200\n",
      "EPOCH: 8, Training cost: 27.803, Validation cost: 36.381, Time: 228\n",
      "EPOCH: 9, Training cost: 26.669, Validation cost: 36.421, Time: 257\n",
      "EPOCH: 10, Training cost: 25.409, Validation cost: 36.223, Time: 285\n",
      "EPOCH: 11, Training cost: 24.605, Validation cost: 35.893, Time: 313\n",
      "EPOCH: 12, Training cost: 24.158, Validation cost: 36.869, Time: 341\n",
      "EPOCH: 13, Training cost: 23.130, Validation cost: 37.881, Time: 370\n",
      "EPOCH: 14, Training cost: 26.859, Validation cost: 35.819, Time: 398\n",
      "EPOCH: 15, Training cost: 24.429, Validation cost: 36.114, Time: 426\n",
      "EPOCH: 16, Training cost: 23.236, Validation cost: 36.642, Time: 455\n",
      "EPOCH: 17, Training cost: 22.227, Validation cost: 37.073, Time: 483\n",
      "EPOCH: 18, Training cost: 23.100, Validation cost: 35.807, Time: 511\n",
      "EPOCH: 19, Training cost: 23.449, Validation cost: 35.545, Time: 540\n",
      "EPOCH: 20, Training cost: 21.750, Validation cost: 36.302, Time: 568\n",
      "EPOCH: 21, Training cost: 20.896, Validation cost: 36.917, Time: 596\n",
      "EPOCH: 22, Training cost: 20.185, Validation cost: 37.114, Time: 625\n",
      "EPOCH: 23, Training cost: 20.134, Validation cost: 37.441, Time: 653\n",
      "EPOCH: 24, Training cost: 19.349, Validation cost: 37.890, Time: 682\n",
      "EPOCH: 25, Training cost: 18.816, Validation cost: 38.238, Time: 710\n",
      "EPOCH: 26, Training cost: 19.667, Validation cost: 37.045, Time: 738\n",
      "EPOCH: 27, Training cost: 20.026, Validation cost: 37.506, Time: 767\n",
      "EPOCH: 28, Training cost: 20.179, Validation cost: 37.537, Time: 795\n",
      "EPOCH: 29, Training cost: 19.182, Validation cost: 37.354, Time: 824\n",
      "EPOCH: 30, Training cost: 19.317, Validation cost: 35.873, Time: 852\n",
      "EPOCH: 31, Training cost: 19.814, Validation cost: 37.053, Time: 880\n",
      "EPOCH: 32, Training cost: 18.177, Validation cost: 37.806, Time: 909\n",
      "EPOCH: 33, Training cost: 17.437, Validation cost: 38.121, Time: 937\n",
      "EPOCH: 34, Training cost: 16.891, Validation cost: 38.882, Time: 966\n",
      "EPOCH: 35, Training cost: 16.584, Validation cost: 39.344, Time: 994\n",
      "EPOCH: 36, Training cost: 16.575, Validation cost: 38.849, Time: 1023\n",
      "EPOCH: 37, Training cost: 16.367, Validation cost: 39.188, Time: 1051\n",
      "EPOCH: 38, Training cost: 15.653, Validation cost: 39.836, Time: 1079\n",
      "EPOCH: 39, Training cost: 16.080, Validation cost: 39.918, Time: 1108\n",
      "EPOCH: 40, Training cost: 16.644, Validation cost: 40.127, Time: 1136\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "n_epochs = 40\n",
    "batch_size = 128\n",
    "n_batches = len(train_X) // batch_size\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "result=[]\n",
    "for epoch in range(n_epochs):\n",
    "    # train\n",
    "    train_costs = []\n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        train_X_mb = np.array(pad_sequences(train_X[start:end], padding='post', value=-1))\n",
    "        train_y_mb = np.array(pad_sequences(train_y[start:end], padding='post', value=-1))\n",
    "\n",
    "        _, train_cost = sess.run([train, cost], feed_dict={x: train_X_mb, d: train_y_mb})\n",
    "        train_costs.append(train_cost)\n",
    "\n",
    "    # valid\n",
    "    #valid_X_mb = np.array(pad_sequences(valid_X, padding='post', value=-1))\n",
    "    #valid_y_mb = np.array(pad_sequences(valid_y, padding='post', value=-1))\n",
    "\n",
    "    #valid_cost = sess.run(cost, feed_dict={x: valid_X_mb, d: valid_y_mb})\n",
    "    test_batch_size = 128\n",
    "    n_batches_test = -(-len(test_X) // test_batch_size)\n",
    "    test_costs = []\n",
    "        \n",
    "    for i in range(n_batches_test):\n",
    "        start = i * test_batch_size\n",
    "        end = start + test_batch_size if (start + test_batch_size) < len(test_X) else len(test_X)\n",
    "            \n",
    "        test_X_padded = np.array(pad_sequences(test_X[start:end], padding='post', value=-1))\n",
    "        test_y_padded = np.array(pad_sequences(test_y[start:end], padding='post', value=-1))\n",
    "            \n",
    "        test_cost = sess.run(cost, feed_dict={x: test_X_padded, d: test_y_padded})\n",
    "        test_costs.append(test_cost)\n",
    "    print('EPOCH: %i, Training cost: %.3f, Validation cost: %.3f, Time: %i' % (epoch+1, np.mean(train_costs), np.mean(test_costs), time.time()-start_time))\n",
    "    result.append(np.mean(test_costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e4486d55c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "翻訳文の生成には`while`ループを使うので, まず`tf`におけるwhileループの実装である`tf.while_loop`について説明します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  5.1. `tf.while_loop`関数  \\[[link\\]](https://www.tensorflow.org/api_docs/python/tf/while_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主な引数は以下のとおりです.\n",
    "\n",
    "- 第1引数 `cond`: `True` or `False` を返す関数 (正確にはcallable)\n",
    "- 第2引数 `body`: 各iterationで実行する関数 (正確にはcallable)\n",
    "- 第3引数 `loop_vars`: `cond`及び`body`に最初に渡される変数\n",
    "\n",
    "`cond`で指定された関数の戻り値が`True`である限り`body`で指定された関数を実行し続けます. そして, `loop_vars`で指定された全ての変数に対して, 最後のiteration後の値を返します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば, 入力が5未満であるかぎり1ずつ足す処理を実行したい場合, コードは次のようになります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "g0 = tf.Graph() # Encoder-Decoderモデルのグラフと区別するために新しいグラフオブジェクトを作成\n",
    "\n",
    "def cond(z):\n",
    "    return z < 5\n",
    "\n",
    "def body(z):\n",
    "    return z + 1\n",
    "\n",
    "with g0.as_default():\n",
    "    z = tf.constant(0)\n",
    "\n",
    "    res = tf.while_loop(cond, body, [z])\n",
    "\n",
    "with tf.Session(graph=g0) as sess_g0:\n",
    "    print(sess_g0.run(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また, `tf.while_loop`の各iteration後の変数の`shape`はデフォルトでは同じであることが指定されています. なので, 各iteration後の`shape`が変化する場合は, この条件を緩和する必要があります.\n",
    "\n",
    "たとえば, 上の1ずつ足すプログラムですべてのiteration後の値を保持して返したい場合, 各iterationの戻り値は\n",
    "\n",
    "```\n",
    "[1], [1, 2], [1, 2, 3], [1, 2, 3, 4], ...\n",
    "```\n",
    "となっていくので, それぞれの`shape`は,\n",
    "```\n",
    "(1,), (2,), (3,), (4,), ...\n",
    "```\n",
    "と変化していきます. つまり, この例ではベクトルの次元数が変化していくので, `shape_invariants`で`shape`を`[None]` (実際は`tf.TensorShape([None])`と指定します.\n",
    "\n",
    "具体的なコードは次のようになります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.]\n"
     ]
    }
   ],
   "source": [
    "g1 = tf.Graph() # Encoder-Decoderモデルのグラフと区別するために新しいグラフオブジェクトを作成\n",
    "\n",
    "def cond(z):\n",
    "    return z[-1] < 5\n",
    "\n",
    "def body(z):\n",
    "    return tf.concat([z, z[-1:]+1], axis=0)\n",
    "\n",
    "with g1.as_default():\n",
    "    z = tf.zeros(1)\n",
    "\n",
    "    res = tf.while_loop(\n",
    "        cond,\n",
    "        body,\n",
    "        [z],\n",
    "        shape_invariants=[tf.TensorShape([None])]\n",
    "    )\n",
    "\n",
    "with tf.Session(graph=g1) as sess_g1:\n",
    "    print(sess_g1.run(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2x1行列に対して同じような操作をしたい場合は次のようになります. この場合各iteration後の行列の`shape`は\n",
    "```\n",
    "(2, 1), (2, 2), (2, 3), (2, 4), ...\n",
    "```\n",
    "と列数のみ変化していくので, `shape_invariants`には`tf.TensorShape([2, None])`と指定します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.]\n",
      " [ 0.  1.  2.  3.  4.  5.]]\n"
     ]
    }
   ],
   "source": [
    "g2 = tf.Graph() # Encoder-Decoderモデルのグラフと区別するために新しいグラフオブジェクトを作成\n",
    "\n",
    "\n",
    "def cond(z):\n",
    "    return tf.reduce_sum(z[:, -1]) < 5*2\n",
    "\n",
    "def body(z):\n",
    "    return tf.concat([z, z[:, -1:]+1], axis=1)\n",
    "\n",
    "with g2.as_default():\n",
    "    z = tf.zeros([2, 1])\n",
    "\n",
    "    res = tf.while_loop(\n",
    "        cond,\n",
    "        body,\n",
    "        [z],\n",
    "        shape_invariants=[tf.TensorShape([2, None])]\n",
    "    )\n",
    "\n",
    "with tf.Session(graph=g2) as sess_g2:\n",
    "    print(sess_g2.run(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "詳細は公式のドキュメントを参照してください.\n",
    "\n",
    "- tf.while_loop: https://www.tensorflow.org/api_docs/python/tf/while_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. グラフの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "未知のデータに対してEncoder-Decoderモデルを適用するとき, 正解ラベル$d$はわからないので, 代わりに前のステップで予測した単語を各ステップでの入力とします. そして, 系列の終わりを表す単語 (`</s>`) が出力されるまで繰り返します.\n",
    "\n",
    "具体的には, $h_{t-1}, c_{t-1}, y_{t-1}$を入力として$y_t$を受け取る操作を, バッチ内の全てのサンプルにおける$y_t$が`</s>`となるまで続けます.\n",
    "\n",
    "毎iterationで1つのステップについてのみ順伝播を計算すれば良いので, ここで各クラスの`f_prop_test`関数を使用します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t_0 = tf.constant(0)\n",
    "y_0 = tf.placeholder(tf.int32, [None, None], name='y_0')\n",
    "h_0 = tf.placeholder(tf.float32, [None, None], name='h_0')\n",
    "c_0 = tf.placeholder(tf.float32, [None, None], name='c_0')\n",
    "f_0 = tf.cast(tf.zeros_like(y_0[:, 0]), dtype=tf.bool) # バッチ内の各サンプルに対して</s>が出たかどうかのflag\n",
    "f_0_size = tf.reduce_sum(tf.ones_like(f_0, dtype=tf.int32))\n",
    "max_len = tf.placeholder(tf.int32, name='max_len') # iterationの繰り返し回数の限度\n",
    "\n",
    "def f_props_test(layers, x_t):\n",
    "    for layer in layers:\n",
    "        x_t = layer.f_prop_test(x_t)\n",
    "    return x_t\n",
    "\n",
    "def cond(t, h_t, c_t, y_t, f_t):\n",
    "    num_true = tf.reduce_sum(tf.cast(f_t, tf.int32)) # Trueの数\n",
    "    unfinished = tf.not_equal(num_true, f_0_size)\n",
    "    return tf.logical_and(t+1 < max_len, unfinished)\n",
    "\n",
    "def body(t, h_tm1, c_tm1, y, f_tm1):\n",
    "    y_tm1 = y[:, -1]\n",
    "\n",
    "    decoder_pre[1].h_0 = h_tm1\n",
    "    decoder_pre[1].c_0 = c_tm1\n",
    "    h_t, c_t = f_props_test(decoder_pre, y_tm1)\n",
    "    y_t = tf.cast(tf.argmax(f_props_test(decoder_post, h_t), axis=1), tf.int32)\n",
    "\n",
    "    y = tf.concat([y, y_t[:, np.newaxis]], axis=1)\n",
    "\n",
    "    f_t = tf.logical_or(f_tm1, tf.equal(y_t, 1)) # flagの更新\n",
    "\n",
    "    return [t+1, h_t, c_t, y, f_t]\n",
    "\n",
    "res = tf.while_loop(\n",
    "    cond,\n",
    "    body,\n",
    "    loop_vars=[t_0, h_0, c_0, y_0, f_0],\n",
    "    shape_invariants=[\n",
    "        t_0.get_shape(),\n",
    "        tf.TensorShape([None, None]),\n",
    "        tf.TensorShape([None, None]),\n",
    "        tf.TensorShape([None, None]),\n",
    "        tf.TensorShape([None])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. 初期値$h_0, c_0, y_0$の獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_X_mb = pad_sequences(valid_X, padding='post', value=-1)\n",
    "_y_0 = np.zeros_like(valid_X, dtype='int32')[:, np.newaxis]\n",
    "_h_enc, _c_enc = sess.run([h_enc, c_enc], feed_dict={x: valid_X_mb})\n",
    "_h_0 = _h_enc[:, -1, :]\n",
    "_c_0 = _c_enc[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. 生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, _, _, pred_y, _ = sess.run(res, feed_dict={\n",
    "    y_0: _y_0,\n",
    "    h_0: _h_0,\n",
    "    c_0: _c_0,\n",
    "    max_len: 100\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4. 生成例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の文: get back , get back .\n",
      "生成文された文: そう な り ま し た 。\n",
      "正解文: 戻 っ て お い で 戻 っ て お い で よ 。\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "\n",
    "origy = valid_X[num][1:-1]\n",
    "predy = list(pred_y[num])\n",
    "truey = valid_y[num][1:-1]\n",
    "\n",
    "print('元の文:', ' '.join([e_i2w[com] for com in origy]))\n",
    "print('生成文された文:', ' '.join([j_i2w[com] for com in predy[1:predy.index(1)]]))\n",
    "print('正解文:', ' '.join([j_i2w[com] for com in truey]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
